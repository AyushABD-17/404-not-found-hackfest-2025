{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67393732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('tweets_sentiment.csv')\n",
    "tweets = df['Tweet'].tolist()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'http\\S+|@\\w+|#|[\\W_]', ' ', str(text))  # Remove URLs, mentions, hashtags, and punctuation\n",
    "    text = text.lower().strip()\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]  # Remove stopwords and short tokens\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_tweets = [preprocess(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1213da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prant\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(processed_tweets, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a263656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\prant\\anaconda3\\envs\\aienv\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to numpy for clustering\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "# Cluster tweets (example: 5 clusters)\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e59e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Meaningful Words:\n",
      "         Keyword  Frequency\n",
      "1           war2          5\n",
      "2         trisha          2\n",
      "3   hardikpandya          2\n",
      "4            ntr          1\n",
      "5          actor          1\n",
      "6           lead          1\n",
      "7         pathan          1\n",
      "8        percent          1\n",
      "9          cameo          1\n",
      "10           100          1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extract top keywords per cluster\n",
    "top_keywords = []\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_tweets = [processed_tweets[i] for i, c in enumerate(clusters) if c == cluster_id]\n",
    "    \n",
    "    # Compute TF-IDF for the cluster\n",
    "    tfidf = TfidfVectorizer(max_features=50)\n",
    "    tfidf_matrix = tfidf.fit_transform(cluster_tweets)\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    # Get top 10 words by TF-IDF score\n",
    "    scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    keywords = [feature_names[i] for i in sorted_indices[:10]]\n",
    "    top_keywords.extend(keywords)\n",
    "\n",
    "\n",
    "# Rank keywords globally and create DataFrame\n",
    "from collections import Counter\n",
    "keyword_counter = Counter(top_keywords)\n",
    "most_meaningful_words = keyword_counter.most_common(10)\n",
    "\n",
    "# Convert to DataFrame\n",
    "keywords_df = pd.DataFrame(most_meaningful_words, columns=['Keyword', 'Frequency'])\n",
    "keywords_df.index += 1  # Start index at 1 for readability\n",
    "\n",
    "print(\"\\nTop 10 Most Meaningful Words:\")\n",
    "print(keywords_df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0af275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "Top 10 Meaningful Phrases (Words and Bigrams)\n",
      "=======================================================\n",
      "       Phrase     Frequency\n",
      "1           war2      5    \n",
      "2         trisha      2    \n",
      "3      war2 lead      1    \n",
      "4            ntr      1    \n",
      "5      actor ntr      1    \n",
      "6          actor      1    \n",
      "7           lead      1    \n",
      "8     lead actor      1    \n",
      "9         pathan      1    \n",
      "10  pathan cameo      1    \n"
     ]
    }
   ],
   "source": [
    "# ... (previous code for preprocessing and clustering)\n",
    "\n",
    "# Extract phrases and sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert embeddings to numpy upfront (add this line)\n",
    "embeddings_np = embeddings.cpu().numpy()  # If using GPU, otherwise just .numpy()\n",
    "\n",
    "# Initialize containers\n",
    "cluster_keyphrases = []\n",
    "cluster_sentences = []\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_indices = [i for i, c in enumerate(clusters) if c == cluster_id]\n",
    "    cluster_processed = [processed_tweets[i] for i in cluster_indices]\n",
    "    cluster_original = [df['Tweet'].iloc[i] for i in cluster_indices]\n",
    "    \n",
    "    # ======================================\n",
    "    # 1. Extract Keyphrases (n-grams)\n",
    "    # ======================================\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=50)\n",
    "    tfidf_matrix = tfidf.fit_transform(cluster_processed)\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    keyphrases = [feature_names[i] for i in sorted_indices[:10]]\n",
    "    cluster_keyphrases.extend(keyphrases)\n",
    "    \n",
    "    # ======================================\n",
    "    # 2. Find Representative Sentence (FIXED)\n",
    "    # ======================================\n",
    "    # Use embeddings_np instead of embeddings tensor\n",
    "    cluster_embeddings = embeddings_np[cluster_indices]\n",
    "    \n",
    "    # Calculate centroid using numpy\n",
    "    centroid = np.mean(cluster_embeddings, axis=0)\n",
    "    \n",
    "    # Find most similar sentence\n",
    "    similarities = cosine_similarity([centroid], cluster_embeddings)\n",
    "    most_representative_idx = np.argmax(similarities)\n",
    "    representative_sentence = cluster_original[most_representative_idx]\n",
    "    \n",
    "    cluster_sentences.append(representative_sentence)\n",
    "\n",
    "# ... (rest of the DataFrame creation code remains the same)\n",
    "\n",
    "# ======================================\n",
    "# Create Final DataFrame\n",
    "# ======================================\n",
    "# Top phrases across all clusters\n",
    "phrase_counter = Counter(cluster_keyphrases)\n",
    "top_phrases = phrase_counter.most_common(10)\n",
    "\n",
    "# Cluster-wise representative sentences\n",
    "cluster_results = []\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_results.append({\n",
    "        'Cluster': cluster_id + 1,\n",
    "        'Top Phrases': \", \".join([p for p, _ in phrase_counter.most_common(5)]),\n",
    "        'Representative Sentence': cluster_sentences[cluster_id]\n",
    "    })\n",
    "\n",
    "# Create DataFrames\n",
    "phrases_df = pd.DataFrame(top_phrases, columns=['Phrase', 'Frequency'])\n",
    "phrases_df.index += 1\n",
    "\n",
    "clusters_df = pd.DataFrame(cluster_results)\n",
    "clusters_df.index += 1\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"Top 10 Meaningful Phrases (Words and Bigrams)\")\n",
    "print(\"=\"*55)\n",
    "print(phrases_df.to_string(index=True, justify='center'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
